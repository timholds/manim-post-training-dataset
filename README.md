# Manim Code Generation with Fine-tuned Models

This project fine-tunes various code generation models on a curated Manim dataset to generate high-quality Manim animation code from natural language descriptions.

## Overview

- **Purpose**: Create a model-agnostic dataset for fine-tuning code generation models on Manim animations
- **Current Dataset**: 4,000+ unique animation examples from 11 high-quality sources
- **Architecture**: Plugin-based data pipeline for easy extensibility
- **Training Method**: QLoRA with Unsloth optimizations  
- **Supported Models**: QWEN2.5-Coder, CodeLlama, DeepSeek, CodeGemma, Stable Code
- **Deployment**: Ollama-compatible GGUF format

## Quick Start

### Prerequisites
- NVIDIA GPU with 16GB+ VRAM
- Python 3.8+
- Ollama (for deployment)

### Installation

```bash
# Clone repository
git clone https://github.com/timholds/manim-post-training-dataset
cd manim-post-training-dataset

# Create and activate virtual environment
python -m venv manim-env
source manim-env/bin/activate

# Install dependencies
uv pip install -r requirements.txt
```

### Prepare Dataset

```bash
# Download and process all datasets with deduplication
python prepare_data.py

# List available data sources
python prepare_data.py --list-sources

# Process specific datasets
python prepare_data.py --sources bespoke_manim thanks_dataset

# Enable data augmentation (creates variations of prompts)
python prepare_data.py --augmentation
```

### Enhance with LLM Descriptions (Optional)

```bash
# Generate better descriptions using LLM (with caching)
python prepare_data_with_llm.py generate-descriptions \
    --input data_formatted/train.json \
    --output data_enhanced/train.json \
    --llm gemini

# Check LLM cache statistics
python prepare_data_with_llm.py cache-stats
```

### Train Model

```bash
# Fine-tune with default settings
python fine_tune.py --model "Qwen/Qwen2.5-Coder-1.5B-Instruct"

# Custom training parameters
python fine_tune.py --model "Qwen/Qwen2.5-Coder-1.5B-Instruct" --epochs 5 --batch-size 2
```

## Documentation

- **[Current Dataset State](docs/CURRENT_STATE.md)** - Latest statistics and quality metrics
- **[Pipeline Architecture](docs/PIPELINE_ARCHITECTURE.md)** - Decoupled pipeline design
- **[Data Pipeline Guide](docs/PIPELINE.md)** - How to use and extend the data pipeline
- **[Adding New Data Sources](docs/migration_guide.md)** - Guide for creating new extractors
- **[Development Roadmap](docs/ROADMAP.md)** - Plans for adding 200-500+ more samples

## Key Features

- ğŸ”Œ **Plugin-based architecture** - Add new data sources by creating a single extractor file
- ğŸ” **Automatic deduplication** - Intelligent duplicate removal across multiple sources
- ğŸ“Š **Model-agnostic dataset** - Handles special tokens during training
- ğŸ¤– **LLM-enhanced descriptions** - Optional LLM generation with smart caching
- ğŸ“ **Decoupled pipeline** - Each stage (extract â†’ enhance â†’ train) is independent
- ğŸš€ **Efficient training** - 4-bit quantized training with ~12GB VRAM usage
- ğŸ§ª **Comprehensive evaluation** - Weights & Biases integration for metrics
- ğŸ”§ **Easy deployment** - Export to Ollama-compatible GGUF format

## Project Structure

```
manim-post-training/
â”œâ”€â”€ prepare_data.py          # Main data preparation script
â”œâ”€â”€ fine_tune.py             # Universal training script
â”œâ”€â”€ extractors/              # Plugin-based data extractors
â”‚   â”œâ”€â”€ base.py             # Base extractor interface
â”‚   â”œâ”€â”€ registry.py         # Dynamic extractor registry
â”‚   â””â”€â”€ sources/            # Individual data source extractors
â”‚       â”œâ”€â”€ kaggle.py       # Kaggle datasets (Manimbench)
â”‚       â”œâ”€â”€ huggingface.py  # HuggingFace datasets
â”‚       â””â”€â”€ local.py        # Local file extractors
â”œâ”€â”€ docs/                    # Detailed documentation
â”‚   â”œâ”€â”€ CURRENT_STATE.md    # Dataset statistics
â”‚   â”œâ”€â”€ PIPELINE.md         # Pipeline usage guide
â”‚   â”œâ”€â”€ migration_guide.md  # Adding new extractors
â”‚   â””â”€â”€ ROADMAP.md          # Future development plans
â”œâ”€â”€ data_formatted/          # Processed datasets
â”œâ”€â”€ models/                  # Trained model outputs
â””â”€â”€ requirements.txt         # Python dependencies
```

## Adding New Data Sources

Adding a new data source is as simple as creating a new extractor:

```python
# extractors/sources/your_source.py
from ..base import BaseExtractor
from ..registry import register_extractor

@register_extractor
class YourSourceExtractor(BaseExtractor):
    source_id = "your_source"
    source_name = "Your Data Source"
    priority = 3  # 1-5, higher = keep when deduplicating
    
    def extract(self):
        # Your extraction logic here
        yield {"description": "...", "code": "..."}
```

See the [Adding New Data Sources](docs/migration_guide.md) guide for detailed instructions.

## Contributing

See the [Development Roadmap](docs/ROADMAP.md) for priority datasets to add. The plugin-based architecture makes it easy to contribute new data sources.

## Deduplication Strategy
Besides rows that have a placeholder for the description that an LLM will fill in later, all descriptions must be unique. When we find two or more rows with the same description, we keep the one with the highest priority source. 